import gradio as gr
import torch
from transformers import BloomForCausalLM, BloomTokenizerFast
from transformers_stream_generator import init_stream_support
init_stream_support()


device = torch.device("cuda:0")
tokenizer = BloomTokenizerFast.from_pretrained(
    "/data/home/ze.song/models/chatbot/chatbot_bloom_1b7",
)

model = BloomForCausalLM.from_pretrained(
    "/data/home/ze.song/models/chatbot/chatbot_bloom_1b7",
    # torch_dtype=torch.float16,
)

model.half()
model.to(device)


def talk(x, tokenizer, model, gpu=True):
    res = []
    max_length = 1024
    inputs = tokenizer.bos_token + x
    input_ids = tokenizer.encode(inputs, return_tensors="pt")
    _, input_len = input_ids.shape
    if input_len >= max_length - 3:
        response = "对话超过字数限制,请重新开始."

        for token in response :
            res.append(token)
            yield ''.join(res)
    if gpu:
        input_ids = input_ids.to(device)
    g = model.generate(
        input_ids,
        eos_token_id=2,
        pad_token_id=3,
        bos_token_id=1,
        do_sample=True,
        temperature=0.6,
        top_p=0.8,
        max_length=512,
        repetition_penalty=1.2,
        do_stream=True,
    )
    for id in g :
        res.append(id.item())
        r = tokenizer.decode(pred, skip_special_tokens=True)
        yield(r)




def add_text(history, text):
    history = history + [(text, None)]
    return history, ""


def bot_streaming(history):
    prompt = ""
    for i, h in enumerate(history):
        prompt = prompt + "\nHuman: " + h[0]
        if i != len(history) - 1:
            prompt = prompt + "\nAssistant: " + h[1]
        else:
            prompt = prompt + "\nAssistant: "

    response_stream = talk(prompt, tokenizer, model)
    for chunk in response_stream:
        history[-1][1] = chunk
        yield history


def add_text_streaming(history, text):
    history = history + [(text, None)]
    return bot_streaming(history)


# 使用生成器进行流式输出
def chat_streaming(history):
    gr_chatbot = gr.Chatbot([], elem_id="chatbot").style(height=300)

    txt = gr.Textbox(
        show_label=False,
        placeholder="Enter text and press enter",
    ).style(container=False)
    with gr.Row():
        clear = gr.Button("重新开始新的对话")
        regen = gr.Button("重新生成当前回答")

    # func
    def on_submit(history, txt, gr_chatbot):
        for new_history in add_text_streaming(history, txt):
            gr_chatbot.update(new_history)

    txt.submit(on_submit, [history, txt, gr_chatbot], [gr_chatbot]).then(
        None, None, None
    )

    def on_clear(gr_chatbot):
        gr_chatbot.update([])

    clear.click(on_clear, [gr_chatbot], queue=False)

    def on_regen(gr_chatbot):
        history = gr_chatbot.data
        for new_history in bot_streaming(history):
            gr_chatbot.update(new_history)

    regen.click(on_regen, [gr_chatbot], queue=False)

    return gr_chatbot


iface = gr.Interface(
    fn=chat_streaming,
    inputs=[
        gr.Interface.Text("初始对话", default="你好"),
        gr.Interface.Function(lambda: talk, live=True, capture_session=True),
    ],
    outputs=gr.Interface.Group([gr.Interface.Textbox("chatbot", type="readonly", height=300)]),
    title="Chatbot",
)

iface.launch(server_name="0.0.0.0", debug=True, server_port=7862)

# if __name__ == "__main__":
#     demo.launch(server_name="0.0.0.0", debug=True, server_port=7862)
